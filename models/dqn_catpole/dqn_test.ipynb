{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Enables TensorFlow 2 behaviors.\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_args_train():\n",
    "    \"\"\"Parse DQN training arguments.\n",
    "    \n",
    "    Returns:\n",
    "        args: The parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--seed',\n",
    "        dest='seed',\n",
    "        type=int,\n",
    "        help='Seed for numpy and tensorflow.',\n",
    "        default=123)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--num_iterations',\n",
    "        dest='num_iterations',\n",
    "        type=int,\n",
    "        help=' Training will end after n number of interations.',\n",
    "        default=20000)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--initial_collect_steps',\n",
    "        dest='initial_collect_steps',\n",
    "        type=int,\n",
    "        help='Exploratory steps.',\n",
    "        default=1000)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--collect_steps_per_iteration',\n",
    "        dest='collect_steps_per_iteration',\n",
    "        type=int,\n",
    "        help='Collected steps per iteration.',\n",
    "        default=1)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--replay_buffer_max_length',\n",
    "        dest='replay_buffer_max_length',\n",
    "        type=int,\n",
    "        help='Size of the replay buffer.',\n",
    "        default=100000)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        dest='batch_size',\n",
    "        type=int,\n",
    "        help='The assets directory.',\n",
    "        default=64)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--lr',\n",
    "        dest='learning_rate',\n",
    "        type=float,\n",
    "        help='The learning rate',\n",
    "        default=1e-3)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--log_interval',\n",
    "        dest='log_interval',\n",
    "        type=int,\n",
    "        help='Output logs after n steps.',\n",
    "        default=200)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--num_eval_episodes',\n",
    "        dest='num_eval_episodes',\n",
    "        type=int,\n",
    "        help='.',\n",
    "        default=10)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--eval_interval',\n",
    "        dest='eval_interval',\n",
    "        type=int,\n",
    "        help='.',\n",
    "        default=1000)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes):\n",
    "    \"\"\"Computes the average return.\n",
    "    \n",
    "    Args:\n",
    "        environment: The environment.\n",
    "        policy: The agent's policy.\n",
    "        num_episodes: Number of episodes.\n",
    "        \n",
    "    Returns:\n",
    "        avg_return: The average return.\n",
    "    \"\"\"\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    \"\"\" Collects data from one step and stores it in the replay buffer.\n",
    "    \n",
    "    Args:\n",
    "        environment: The environment.\n",
    "        policy: The agent's policy.\n",
    "        buffer: The replay buffer.\n",
    "        \n",
    "    Yields:\n",
    "        A trajectory added to the replay buffer.\n",
    "    \"\"\"\n",
    "        \n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    buffer.add_batch(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(environment, policy, buffer, n_steps):\n",
    "    \"\"\" Collects data from n steps and stores it in the replay buffer.\n",
    "    \n",
    "    Args:\n",
    "        environment: The environment.\n",
    "        policy: The agent's policy.\n",
    "        buffer: The replay buffer.\n",
    "        n_steps: The number of steps to collect data.\n",
    "        \n",
    "    Yields:\n",
    "        n_steps added to the replay buffer.\n",
    "    \"\"\"\n",
    "    for _ in range(n_steps):\n",
    "        collect_step(environment, policy, buffer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, filename, eval_env, eval_py_env, num_episodes=5, fps=30):\n",
    "    filename = filename + \".mp4\"\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            video.append_data(eval_py_env.render())\n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                video.append_data(eval_py_env.render())\n",
    "    return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(env_name=\"CartPole-v0\"):\n",
    "    \n",
    "    \n",
    "    env = suite_gym.load(env_name)\n",
    "    env.reset()\n",
    "\n",
    "    train_py_env = suite_gym.load(env_name)\n",
    "    eval_py_env = suite_gym.load(env_name)\n",
    "    train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "    \n",
    "    return train_env, eval_env, train_py_env, eval_py_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(train_env):\n",
    "    \n",
    "    fc_layer_params = (100,)\n",
    "    q_net = q_network.QNetwork(\n",
    "        train_env.observation_spec(),\n",
    "        train_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "    \n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(train_env, q_net, learning_rate):\n",
    "    \n",
    "    \n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    train_step_counter = tf.Variable(0)\n",
    "\n",
    "    # Create the agent\n",
    "    agent = dqn_agent.DqnAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "    agent.initialize()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policies(train_env, agent):\n",
    "    \n",
    "    eval_policy = agent.policy\n",
    "    collect_policy = agent.collect_policy\n",
    "    random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "    \n",
    "    return eval_policy, collect_policy, random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay_buffer(train_env, agent, random_policy, replay_buffer_max_length,\n",
    "                        initial_collect_steps, batch_size):\n",
    "    \n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "    collect_data(train_env, random_policy, replay_buffer, n_steps=initial_collect_steps)\n",
    "\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3, \n",
    "        sample_batch_size=batch_size, \n",
    "        num_steps=2).prefetch(3)\n",
    "\n",
    "    iterator = iter(dataset)\n",
    "    \n",
    "    return replay_buffer, iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_train_eval():\n",
    "    \n",
    "    args = dqn_args_train()\n",
    "    \n",
    "    train_env, eval_env, train_py_env, eval_py_env = create_environment()\n",
    "    q_net = create_network(train_env)\n",
    "    agent = create_agent(train_env, q_net, args.learning_rate)\n",
    "    eval_policy, collect_policy, random_policy = create_policies(train_env, agent)\n",
    "    replay_buffer, iterator = create_replay_buffer(\n",
    "        train_env, \n",
    "        agent, \n",
    "        random_policy, \n",
    "        args.replay_buffer_max_length,\n",
    "        args.initial_collect_steps,\n",
    "        args.batch_size)\n",
    "    \n",
    "    # Set the random seed.\n",
    "    if args.seed is not None:\n",
    "        np.random.seed(args.seed)\n",
    "        tf.random.set_seed(args.seed)\n",
    "\n",
    "    # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "    agent.train = common.function(agent.train)\n",
    "\n",
    "    # Reset the train step\n",
    "    agent.train_step_counter.assign(0)\n",
    "\n",
    "    # Evaluate the agent's policy once before training.\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, args.num_eval_episodes)\n",
    "    returns = [avg_return]\n",
    "\n",
    "    for _ in range(args.num_iterations):\n",
    "\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        for _ in range(args.collect_steps_per_iteration):\n",
    "            collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "        if step % args.eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_env, agent.policy, args.num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            \n",
    "        if step % (args.eval_interval * 5) == 0:\n",
    "            create_policy_eval_video(agent.policy, \"videos/cartpole_\" + str(step), eval_env, eval_py_env)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 38.37812423706055\n",
      "step = 400: loss = 55.52467727661133\n",
      "step = 600: loss = 20.308481216430664\n",
      "step = 800: loss = 22.60995864868164\n",
      "step = 1000: loss = 37.635780334472656\n",
      "step = 1000: Average Return = 41.0\n",
      "step = 1200: loss = 24.170095443725586\n",
      "step = 1400: loss = 62.31285095214844\n",
      "step = 1600: loss = 34.68653106689453\n",
      "step = 1800: loss = 61.01420593261719\n",
      "step = 2000: loss = 4.443548202514648\n",
      "step = 2000: Average Return = 55.29999923706055\n",
      "step = 2200: loss = 16.577068328857422\n",
      "step = 2400: loss = 36.24903869628906\n",
      "step = 2600: loss = 84.85283660888672\n",
      "step = 2800: loss = 30.161462783813477\n",
      "step = 3000: loss = 70.71626281738281\n",
      "step = 3000: Average Return = 45.70000076293945\n",
      "step = 3200: loss = 67.6693344116211\n",
      "step = 3400: loss = 56.20784378051758\n",
      "step = 3600: loss = 133.0223846435547\n",
      "step = 3800: loss = 38.070770263671875\n",
      "step = 4000: loss = 84.8382568359375\n",
      "step = 4000: Average Return = 97.0999984741211\n",
      "step = 4200: loss = 75.252685546875\n",
      "step = 4400: loss = 130.385498046875\n",
      "step = 4600: loss = 128.68849182128906\n",
      "step = 4800: loss = 177.4630126953125\n",
      "step = 5000: loss = 165.8138427734375\n",
      "step = 5000: Average Return = 96.0999984741211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5200: loss = 32.419471740722656\n",
      "step = 5400: loss = 157.88783264160156\n",
      "step = 5600: loss = 141.82223510742188\n",
      "step = 5800: loss = 69.51016998291016\n",
      "step = 6000: loss = 52.09953689575195\n",
      "step = 6000: Average Return = 115.4000015258789\n",
      "step = 6200: loss = 225.6766357421875\n",
      "step = 6400: loss = 153.56661987304688\n",
      "step = 6600: loss = 203.82626342773438\n",
      "step = 6800: loss = 192.7113494873047\n",
      "step = 7000: loss = 90.13687133789062\n",
      "step = 7000: Average Return = 196.8000030517578\n",
      "step = 7200: loss = 94.67214965820312\n",
      "step = 7400: loss = 186.1062469482422\n",
      "step = 7600: loss = 16.9442138671875\n",
      "step = 7800: loss = 512.1292114257812\n",
      "step = 8000: loss = 237.49038696289062\n",
      "step = 8000: Average Return = 183.0\n",
      "step = 8200: loss = 579.2872314453125\n",
      "step = 8400: loss = 465.34539794921875\n",
      "step = 8600: loss = 21.985334396362305\n",
      "step = 8800: loss = 293.43133544921875\n",
      "step = 9000: loss = 167.97677612304688\n",
      "step = 9000: Average Return = 199.8000030517578\n",
      "step = 9200: loss = 210.78369140625\n",
      "step = 9400: loss = 317.59332275390625\n",
      "step = 9600: loss = 221.28619384765625\n",
      "step = 9800: loss = 22.684947967529297\n",
      "step = 10000: loss = 988.832763671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 84.01322937011719\n",
      "step = 10400: loss = 884.515625\n",
      "step = 10600: loss = 217.8933868408203\n",
      "step = 10800: loss = 361.18560791015625\n",
      "step = 11000: loss = 553.664794921875\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 567.9567260742188\n",
      "step = 11400: loss = 1790.2293701171875\n",
      "step = 11600: loss = 445.34130859375\n",
      "step = 11800: loss = 31.82424545288086\n",
      "step = 12000: loss = 82.710693359375\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 37.00714111328125\n",
      "step = 12400: loss = 18.624347686767578\n",
      "step = 12600: loss = 175.9971923828125\n",
      "step = 12800: loss = 166.88958740234375\n",
      "step = 13000: loss = 482.79541015625\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 519.7514038085938\n",
      "step = 13400: loss = 230.82632446289062\n",
      "step = 13600: loss = 35.094215393066406\n",
      "step = 13800: loss = 1824.8187255859375\n",
      "step = 14000: loss = 70.08675384521484\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 1568.5677490234375\n",
      "step = 14400: loss = 42.03715515136719\n",
      "step = 14600: loss = 54.524105072021484\n",
      "step = 14800: loss = 1530.28369140625\n",
      "step = 15000: loss = 1271.54736328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 161.66961669921875\n",
      "step = 15400: loss = 3647.3544921875\n",
      "step = 15600: loss = 2617.25048828125\n",
      "step = 15800: loss = 2542.709716796875\n",
      "step = 16000: loss = 2738.21044921875\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 86.77226257324219\n",
      "step = 16400: loss = 2887.104248046875\n",
      "step = 16600: loss = 2208.19189453125\n",
      "step = 16800: loss = 52.97126007080078\n",
      "step = 17000: loss = 53.186248779296875\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 1218.6190185546875\n",
      "step = 17400: loss = 2900.903564453125\n",
      "step = 17600: loss = 3360.69384765625\n",
      "step = 17800: loss = 70.0076904296875\n",
      "step = 18000: loss = 77.5316162109375\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 178.0081329345703\n",
      "step = 18400: loss = 235.01988220214844\n",
      "step = 18600: loss = 96.42203521728516\n",
      "step = 18800: loss = 5020.37646484375\n",
      "step = 19000: loss = 360.89434814453125\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 1498.7930908203125\n",
      "step = 19400: loss = 139.73745727539062\n",
      "step = 19600: loss = 83.27178192138672\n",
      "step = 19800: loss = 6006.0791015625\n",
      "step = 20000: loss = 124.34381103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 20000: Average Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "dqn_train_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(returns, num_iterations, eval_interval):\n",
    "    iterations = range(0, num_iterations + 1, eval_interval)\n",
    "    plt.plot(iterations, returns)\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_agents",
   "language": "python",
   "name": "tf_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
