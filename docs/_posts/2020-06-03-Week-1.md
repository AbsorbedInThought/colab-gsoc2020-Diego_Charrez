---
title: "Coding period week 1"
excerpt: ""

sidebar:
  nav: "docs"

toc: true
toc_label: "TOC installation"
toc_icon: "cog"


categories:
- GSoC
tags:
- Jderobot

author: Diego Charrez
pinned: false
---

I spent the community bonding period learning more about Jderobot's Behavior Studio which focuses on using deep learning for self driving cars. It was also great to talk to my mentors, they are very kind and supportive, and know more about the Jderobot community.

This first week of the coding period, I started by studying the tensorflow agents library which has a set of great tutorials, and implemented some basic examples with Deep Q-Networks (DQN), Additionally I set up a Dockerfile to ease my work.

Exploring tensorflow agents and DQN in easier environments, would help to translate the implementation to more complex environment like Formula 1 environment.

![Cartpole]({{ "/assets/images/blogs/cartpole.gif" | absolute_url }})

## Deep Q-Networks?

Deep Q-Networks were developed by Deepmind in 2015, combining deep neural networks and Q-learning which is a classic Reinforcement learning algorithm. Q-learning uses the state-value function Q that measures how good is to take an action $a$ from the state $s$ and it is denoted as $Q_{\pi}(s,a)$ \[[1]\](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).

$$\begin{equation}Q^{*}(s, a) = \mathbb{E}\left[ r + \gamma \max_{a'} Q^{*}(s', a') \right]\end{equation}$$

## DQN on Cartpole




## References

[1] Deepmind, [DQN (Deep Q-Network) algorithm](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)